import torch
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import os
import numpy as np

# Import the VulnerabilityDetectorAgent
from src.agent import VulnerabilityDetectorAgent

# Configuration - IMPORTANT: This section is updated for robust local path handling.
script_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(script_dir)
AGENT_AI_DATA_ROOT = os.path.join(project_root, "AgentAI_Data")
DRIVE_TRAINED_MODELS_PATH = os.path.join(AGENT_AI_DATA_ROOT, "trained_models")
TRAINED_MODEL_PATH = os.path.join(DRIVE_TRAINED_MODELS_PATH, "final_model")

MODEL_NAME = "microsoft/codebert-base"
# FastAPI app instance
app = FastAPI(
    title="Code Vulnerability Detector",
    description="API for detecting code vulnerabilities using a fine-tuned CodeBERT model."
)

# Global variables for model, tokenizer, and now the agent
tokenizer = None # For direct /analyze-code/ endpoint
model = None     # For direct /analyze-code/ endpoint
device = None
vulnerability_detector_agent = None # NEW: Global agent instance for /detect-and-mitigate/
# Pydantic model for request body
class CodeInput(BaseModel):
    code_snippet: str

@app.on_event("startup")
async def startup_event():
    """
    Load the tokenizer and model, and initialize the agents when the FastAPI application starts up.
    """
    global tokenizer, model, device, vulnerability_detector_agent
    print("--- FastAPI Application Startup ---")
    try:
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        # --- For the direct /analyze-code/ endpoint ---
        # Load tokenizer (always from base model name for consistency with pre-training)
        print("  Loading base tokenizer for direct classification endpoint...")
        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

        # Check if the TRAINED_MODEL_PATH exists before trying to load
        if not os.path.exists(TRAINED_MODEL_PATH):
            print(f"Error: Trained model directory not found at {TRAINED_MODEL_PATH}.")
            print("Please ensure your 'AgentAI_Data' folder (containing 'trained_models/final_model')")
            print(f"is located at: {AGENT_AI_DATA_ROOT}")
            raise FileNotFoundError(f"Trained model directory not found: {TRAINED_MODEL_PATH}")
        # Load the fine-tuned model for the direct /analyze-code/ endpoint
        print(f"  Loading fine-tuned model for direct classification endpoint from {TRAINED_MODEL_PATH} on device: {device}...")
        model = AutoModelForSequenceClassification.from_pretrained(TRAINED_MODEL_PATH)
        model.to(device)
        model.eval() # Set model to evaluation mode
        print("  Direct prediction model loaded.")

        # --- For the /detect-and-mitigate/ endpoint (initializes the full agent) ---
        print("  Initializing VulnerabilityDetectorAgent for combined detection endpoint...")
        vulnerability_detector_agent = VulnerabilityDetectorAgent() # This agent internally loads its own CodeAnalyzer
        print("--- FastAPI Application Startup Complete ---")
    except FileNotFoundError as fnfe:
        print(f"Startup Error (FileNotFound): {fnfe}")
        raise HTTPException(status_code=500, detail=f"Failed to load AI model during startup: {fnfe}")
    except Exception as e:
        print(f"Startup Error (Unexpected): {e}")
        raise HTTPException(status_code=500, detail=f"An unexpected error occurred during startup: {e}")

@app.post("/analyze-code/")
async def analyze_code(input: CodeInput):
    """
    Analyzes a given code snippet for vulnerabilities using the fine-tuned CodeBERT model (direct classification).
    Returns 1 if vulnerable, 0 if not vulnerable.
    """
    if tokenizer is None or model is None:
        raise HTTPException(status_code=503, detail="AI model not loaded. Please try again later.")

    code_snippet = input.code_snippet

    if not code_snippet:
        raise HTTPException(status_code=400, detail="Code snippet cannot be empty.")

    try:
        inputs = tokenizer(
            code_snippet,
            return_tensors="pt",
            truncation=True,
            max_length=512,
            padding=True
        ).to(device)

        with torch.no_grad():
            outputs = model(**inputs)
            logits = outputs.logits

        prediction = torch.argmax(logits, dim=1).item()
        vulnerability_status = "Vulnerable" if prediction == 1 else "Not Vulnerable"

        return {
            "code_snippet": code_snippet,
            "prediction_raw": prediction,
            "vulnerability_status": vulnerability_status,
            "confidence_scores": logits.softmax(dim=1).tolist()[0]
        }
    except Exception as e:
        print(f"Error during /analyze-code/ processing: {e}")
        raise HTTPException(status_code=500, detail=f"Error during code analysis: {e}")

@app.post("/detect-and-mitigate/") # NEW ENDPOINT FOR COMBINED RESULTS
async def detect_and_mitigate(input: CodeInput):
    """
    Detects vulnerabilities in a code snippet using static analysis and a deep learning model,
    and provides mitigation suggestions from the knowledge base.
    Returns a structured report of findings.
    """
    if vulnerability_detector_agent is None:
        raise HTTPException(status_code=503, detail="Vulnerability Detector Agent not initialized during startup.")

    code_snippet = input.code_snippet

    if not code_snippet:
        raise HTTPException(status_code=400, detail="Code snippet cannot be empty.")

    try:
        # Use the agent's unified method
        results = vulnerability_detector_agent.detect_and_suggest_mitigation(code_snippet)
        return results
    except Exception as e:
        print(f"Error during /detect-and-mitigate/ processing: {e}")
        raise HTTPException(status_code=500, detail=f"Error during combined detection and mitigation: {e}")


if __name__ == "__main__":
    import uvicorn
    print("This script is meant to be run with uvicorn directly.")
    print("Example: uvicorn src.vulnerability_api:app --reload")

